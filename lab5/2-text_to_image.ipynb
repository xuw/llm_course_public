{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## 4. Text-to-Image with Stable Diffusion"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.1 Comfy-UI\n", "\n", "ComfyUI is a powerful modular diffusion model GUI that allows users to easily create and run diffusion models.\n", "\n", "\n", "### Step 1. Running the ComfyUI on the cluster\n", "We have built a Docker image that contains all the necessary dependencies to run ComfyUI. To run ComfyUI, follow the steps below:\n", "\n", "1. Copy and create a new configuation file based on https://github.com/iiisthu/ailab/blob/main/user/comfyui-template.yaml .  You should set your namespace in the configure file, and use harbor-local.ai.iiis.co/llm-course/comfyui:v1 as the Docker image (already set in the template).\n", "\n", "2. Helm install, just as other labs.\n", "\n", "3. In order to access the website your started in the cluster, you should run the following command to forward the port to your local computer.  (you can also do it with the Kubernetes plugin in VSCode).  \n", "```bash\n", "# on your local PC\n", "kubectl port-forward pod/<pod_name> 8188:8188\n", "```\n", "\n", "4. Opne a brower and visit https://127.0.0.1:8188 , you should see your UI.  \n", "\n", "\n", "### Step 2. Playing with the comfy UI.\n", "![ComfyUI Interface](assets/preview.png)\n", "1. Import workflow configuration file. Click the `Workflow` button and select the workflow configuration file stored in **your PC**. We provide some example configs in `LLM-applications-course/lec5/comfy_example_workflows`. You can download to your PC and import them.\n", "![Example Workflow Configuration File](assets/select.png)\n", "\n", "2. Select models. You can enlarge the page to see the workflow more clearly. Here we can see in this workflow, the `Load Checkpoint` module requires the `ckpt_name` should be one of sdv3/2b_1024/sd3_medium.safetensors. For example, we select sd3_medium.safetensors.\n", "\n", "3. Write your text prompt in `CLIP Text Encode` module. `Prompt` is what we want, `Negative Prompt` is what we don't want. \n", "\n", "4. (Optional) Change the `Seed` to `randomize` generate different results.\n", "\n", "5. (Optional) Customize your workflow, e.g., add a lora module.\n", "\n", "6. Click `Queue` to run the workflow.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#### your task ####\n", "# write down your prompts here\n", "# and copy the generated image below\n", "# if you have a different workflow from the above (either you write your own, or you find on the Internet)\n", "# please submit it together with this notebook"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.2 Training Stable Diffusion with WebUI\n", "\n", "stable-diffusion-webui is a web interface for Stable Diffusion. Here we provide a tutorial on how to train Stable Diffusion using the web interface.\n", "\n", "#### Step 1. Running the Stable Diffusion WebUI on the cluster\n", "We have built a Docker image that contains all the necessary dependencies to run sd-WebUI. To run sd-WebUI, follow the steps below:\n", "\n", "Follow a similar procedure to start the webUI, using webui-template.yaml. \n", "This time, please forward port 7860 instead. \n", "\n", "You should be able to access the GUI at https://127.0.0.1:7860  (from your pc)\n", "\n", "#### Step 2. Playing with SD-WebUI\n", "Pay attention to all paths in pictures below. If a path starts with \"/share\", you should replace it with \"/ssdshare/share\", e.g., \"/share/lab5/clip-vit-l-14\" -> \"/ssdshare/share/lab5/clip-vit-l-14\".\n", "\n", "We want to teach the model a `concept` of `headless_statue`.\n", "![Headless Statue](assets/sd-hdst.jpeg)\n", "\n", "1. Select a sd model from the dropdown list.\n", "![Select Model](assets/sd-ckpt.png)\n", "\n", "2. Try to generate images using prompt: \"An oil painting of headless_statue\". We are not satisfied with the generated images.\n", "![Generate Images](assets/sd-base.png)\n", "\n", "3. Preprocess our `headless_statue` images. Set parameters following images below, and click `Generate` button.\n", "![Preprocess Images](assets/sd-preprocess.png)\n", "\n", "4. Create embedding for our concept `headless_statue`. \n", "\n", "- Name: filename for the created embedding. You will also use this text in prompts when referring to the embedding.\n", "\n", "- Initialization text: the embedding you create will initially be filled with vectors of this text. If you create a one vector embedding named \"zzzz1234\" with \"tree\" as initialization text, and use it in prompt without training, then prompt \"a zzzz1234 by monet\" will produce same pictures as \"a tree by monet\".\n", "\n", "- Number of vectors per token: the size of embedding. The larger this value, the more information about subject you can fit into the embedding, but also the more words it will take away from your prompt allowance. With stable diffusion, you have a limit of 75 tokens in the prompt. If you use an embedding with 16 vectors in a prompt, that will leave you with space for 75 - 16 = 59. Also from my experience, the larger the number of vectors, the more pictures you need to obtain good results.\n", "\n", "![Create Embedding](assets/sd-create-emb.png)\n", "\n", "5. Train the embedding. Set parameters following images below, and click `Train Embedding` button.\n", "![Train Embedding](assets/sd-train.png)\n", "![Sample](assets/sd-sample.png)\n", "\n", "6. Generate images using prompt: \"An oil painting of `Name`\". We can see the generated images are more related to our concept `headless_statue`.\n", "![Result](assets/sd-result.png)\n", "\n", "\n", "### Textual Inversion\n", "Textual Inversion is a parameter efficient method to train Stable Diffusion. This method can be used to represent a wide array of concepts. Trained on this method, Stable Diffusion can learn a pseudo-word that represents a specific artist or a new concept.\n", "![Textual Inversion](assets/teaser.JPG)\n", "\n", "#### Why we need Textual Inversion Algorithm?\n", "Suppose we have a sd model (which is trained on a specific dataset without the image of `The Thinker`). When we want to use it to generate `A cat in the pose of The Thinker`, we need rewrite our prompt to `A cat with its hand on its chin, sitting on a rock, its eyes looking down thoughtfully`. This is because the model doesn't know what the exact pose of `The Thinker` is. Textual Inversion can help us to find the pseudo-word that represents the concept of `The Thinker` with 3-5 `The Thinker` images.\n", "\n", "#### How does it work?\n", "The essence of Textual Inversion is to map the object in the image to a pseudo-word(A high dimension vector actually. Not necessarily a natural language word, we barely use natural language word to tag it)\n", "![Principal](assets/training.JPG)\n", "\n", "#### Why not train embedding directly?\n", "The scale of the embedding is $\\frac{vocab\\_ size}{token\\_ vectors\\_ num}$ times of the pseudo-word embedding in Textual Inversion, which requires far more data to train (Where vocab_size is the size of the vocabulary used in stable diffusion, and token_vectors_num is the number of token vectors we have to train in textual inversion)."]}], "metadata": {"language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}