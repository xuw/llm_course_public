########### First thing first ###########
NameSpace: xrw22  # 自己的namespace （同用户名）

########### vLLM Configuration ###########
# vllmPort: 8000
ContainerImage: harbor.ai.iiis.co/xuw/pytorch:v1.5

# Using the llama-3-8B-instruct model
#ModelName: /ssdshare/Meta-Llama-3-8B-Instruct/

ModelName: /ssdshare/xuw/llama-2-7b-py18k_merged/

# Using the llama-3-70B-instruct model, sort of runs with the following, and set DshmSize to 16Gi, but not working well
# ModelName: /ssdshare/Meta-Llama-3-70B-Instruct-awq/
# AdditionalArgs: | 
#     "--trust-remote-code", "--quantization", "awq", "--kv-cache-dtype", "fp8", "--tensor-parallel-size", "2", "--max-model-len", "6432", 

# Or using the Phi-3 model - you will need to add two additional arguments
# ModelName: /ssdshare/Phi-3-mini-128k-instruct/
# AdditionalArgs: | 
#     "--trust-remote-code", "--max-model-len", "7260",

# Resource for the vLLM server
Limits:  
 CPU: 16
 memory: 64Gi
 GPU: 2
DshmSize: 4Gi

# Optional: Replace the default Command and Args

# vLLMCommand: '["python3", "-m", "vllm.entrypoints.openai.api_server"]'
# vLLMArgs: | 
#       ["--model", "/ssdshare/Meta-Llama-3-8B-Instruct/", 
#        "--dtype", "auto",
#        "--api-key", "$VLLM_API_KEY",
#        ]

########### Gradio Configuration ###########

## Replace with your domain name and gradio app docker image
IngressHost: YOUR_DNS_NAME
GradioImage: harbor.ai.iiis.co:9443/YOUR_IMAGE_NAME:VERSION
OpenAIUrl: http://openai-api:8000/v1

## Optional: Replace the default Command and Args
# GradioCommand: ["bash", "-c", "--"]
# GradioArgs: ["while true; do sleep 30; done;"]